#Lecture 1
Homogeneous coordinates extend the standard system with one additional scale dimension. This has desirable properties like being able to combine rotation & translation into a single transformation matrix which operates on homogeneous points by single multiplication. Also this allows to represent lines in a convenient way so we can get line intersection by cross product of two lines and distance to a point by doing a dot product with a line. Converting is just dividing by the scale parameter, the last element in the vector. Pinhole camera model can be explained simply from the projection matrix P = K[R|t] = KT containing camera intrinsic & extrinsic parameters in K & T respectively. Extrinsics represent camera pose in the  world while intrinsics explain how the projection is done to a camera plane.

#Lecture 2
Camera model parameters P = K[R|t]Q = PQ, focal length is distance from pinhole to the image plane. Deltas are shift parameters to set the origin of an image to the upper left corner, bc computers store images in this way. Skew parameters alpha & beta are used in camera matrix if pixels are not rectangular, skewed a bit - this can happen in real world setup.   Distortion is an effect of using real lenses, which are not perfectly representative by pinhole camera model. The lens focuses light beams into a bigger hole to capture more light in reality. So we get these distorted images. OF course we can model this effect by polynomial mode lfor example. And the equation shows how we can calculate the pixel location where to sample distorted image to map it back to straight lines. Homography is point to point mapping from one plane to another. This could be correspondances between two image planes, pixel to pixel mapping. It's used in the camera calibration to estimate the parameters. We can estiamte Homography itself by having set of correspondances between two planes. How many? It's 9 number but one of them is scale so we need 8 constraints. Two points i.e. one pair provide two constrains thus we need at least 4 pairs of corresponding points. The estimation itself can be done with linear algorithm. Formulation is based on the fact that vector q1 = Hq2, so the cross product should be zero. This is equivalent to Ax = 0; essentially it's homogeneous least squares problem. It can be solved by SVD imposing constraints that ||x||_2 = 1.

#Lecture 3
Stereo visio is setup of two cameras spaces by know distance and can be used for traingulation of points in 3D. Both matrices (F,E) map point to their corresponding epipolar lines in the other image. Essential matrix operates on points in 3D points while the Fundamental on theirs projection in 2D image plane. So both form requirements for pixel correspondances because corresponding pixels between images should lay on the corresponding epipolar line. However it can be interpreted in a different way, for instance, during the lecture E was derived from the fact that vector t x Rp1 + t gives a normalvector to epipolar plane. Then requirement can be interpreted that the pixel vector lays in the same plane and their dot product is known to be zero. Fundamental matrix encodes imilar mapping it's just a version with camera matrix introduces and the points are replaced by 2D projections in image plane.
- Essential matrix maps a vector from a principal point to image plane (p1) to the normal of that plane. Sooooo if it's true that Ep1 = normal, then taking a vector in the same epipolar plane and multiplying it with a normal should give zero => p2'Ep1 = p2'n. Beautiful;
- Fundamental matrix is equivalent but operates on 3D points instead of 2D vectors. Basically replacing the 2d points in the p2'Ep1 by their derivation from projection equation. p = K^(-1)q and this gives F = K2^(-T) E K1^(-1). And the normal mapping is q2'Fq1 = 0. Wonderful;
- Both of them form requirements for pixel correspondences : )
Triangulation is process of getting 3D points from 2D pixel correspondances. One point is not enought since we need at least three constraints to solve for a single point, but if we have a pair of pixel cordinates and both projections matrices we can form 4 constraints which is enough to traingulate a point. Linear algorithm is well suited but not perfect. In most cases it's more than enough, then we can pose least squares problem and solve it with SVD, the result is a 3D point - inside the right sigular vector aligned with smallest eigen value of B matrix. The error that linear algorithm is minimizing is not optimal bc it includes scalling parameter s - that is large for cameras futher from the point. Ideal error measure would not include it and we would like to solve that problem with non-linear optimization algorithm which we'll do later.

#Lecture 4
DLT (Direct linear transform) can be used to calibrate a camera. It's the same principle as was used in the previous weeks to estimate homography or fundamental matrices. By calibration here I mean finding extrinsic parameters (ones that transform world coordinates to camera reference frame) and intrinsic (whose transform points from the camera coordinate system to pixel coordinate system). We can estimate projection matrix P by using DLT algorithm. This requires to have n pairs of points (3D => 2D pixels correspondences), single pair introduces 2D constrains on X,Y (the third dim is just scale). How many n? P has degrees of freedom: 5 of camera matrix f,alpha,beta,deltax,deltay, 3 rotation params, 3 translation params, 1 for scale. In total 12 degrees of freedom, having in mind that one pair has 2 constraints we need at least 6 points in order to do that. Zhang's method for camera calibration. If we have a known list of 3D points all lying in a plane we can determine homographies giving the mapping between image plane and the one in 3D space. Because we can create connections between 2D camera pixels and known pattern in free space by knowing predefined pattern (with known distances). Checkerboard is commonly used as a 3D plane but basically any other plane could be used with predefined pattern. First we assume all corners are in the Z = 0 plane. So the points in 3D look like Q = [X,Y,0]. Later we can simplify projection equation by throwing out the third column from transformation matrix (Z = 0). This leaves us with homography mapping q = HQ. As explained before we know the correspondences so we can collect a bunch of sample pairs and solve it by DLT as we did with previously. By some clever math we can extract constraints on camera matrix. When we have K it's trivial to compute transformation parameters. Camera parameters again have 12 degrees of freedom so in theory it should be possible top estimate all of the from 6 pairs. In practice it doesn't give good enough results and it's advised to use way more. Some people do it with 2000+ pictures : ). Calibration can also be frames as non-linear least squares problem optimizing over all the camera & distortion parameters by reducing the distance between observations and projections.

#Lecture 5
In computer vision we encounter many problems that can be phrased as non-linear least squares: homography estimation, camera calibration with distortion model, triangulation. There are many algorithm to choose for the problem but Levenberg-Marquardt is often well suited to the types of problems we are likely to encounter in computer vision. The problem is posed as: error = ||f(x)||^2_2. The step of the algorithm goes as: (J'J + lambda*I)*delta = -J*f(x_k). As lambda is increased we just take a small step towards negative gradient direction. But if it's close to the minimum second order approximation of error is relatively good and we can decrease lambda as we get closer. The easiest way to get a Jacobian of a function is to simply use forward, backward difference although it's not very accurate for the learning purpose it was more than enough. In case analytical form can be derived it would more preferable or one can you automatic differentiation tools, like pytorch or many other so you can get derivatives essentially for free, program tracks the operations under the hood and gradients are provided by extrapolating those simple underlying instructions like multiplication, addition, etc. Also we touched upon rotation representation in this week's lectures. Covered types were: Euler angles - minimal three number representation suffering from gimbal lock so is not very suitable for computations because at singularities we lose degrees of freedom - system can rotate only in two dimensions then. Axis angle - this is used by opencv, represented by a vector and an angle, how much rotation does it have around that axis. Quaternions - are gold standard for making computations on rotations because of nice properties like not having any singularities, the only drawback is that we have to normalize it every time to keep it as a valid quaternion, it has 4 components.

#Lecture 6
Image correspondence is a problem of finding unique pixel matches between two images capturing the same object from a different perspectives. In order for them to be unique they need a reasonable descriptions. Some simple features to identify them: harris corners, canny edges but in practice you would use more robust features SIFT, SURF, etc. To achieve more reasonable result we focus on the area around each point and try to create meaningful area description which is more resilient to noise bc it's computer over larger area - more information. 
Gaussian filter is one of the filtering techniques to smoothen out an image. Also it's useful bc we know analytical derivatives of the function. For example Harris corner detector is based on this. We can find averaged Hessian dependant on Gaussian kernel and corners have large values of the metric (det and trace of C(x,y)) regardless of the direction of travel.

#Lecture 7
We have some techniques to fit models. The simpler one is Hough transform â€“ r,theta representation. Idea: we can sample our original image and draw all possible lines going through that point in Hough Space. Do that for all points, the maximum intensity will represent line in original image. We can generalize to more complex models, but it becomes impractical for more than three degrees. RANSAC is based on assumption that by trying to randomly sample some data points and trying to fit the model on each sample we eventually will arrive at some reasonable model which represents the data overall. The approach is outlier resistant and very robust.

#Lecture 8
BLOB - Binary large objects are basically dark areas surrounded by brighter intensities or other way around bright areas surrounded by dark ones. For identifying these second order derivative or just Hessian can be used to determine the curvature of intensities at a point. Similar to Harris corner detector the measure could be represented by trace or determinant of Hessian. In this case we use trace(H) and this can be approximated by difference of Gaussians (DoG). This is a method of taking different levels of blur with Gaussian kernel over the same image and later subtracting those scales, taking out extrema, use non-maximum suppression to make it more robust and woahhla! SIFT features are more complicated, we have not implemented those, but BLOB detection is the first step in that algorithm. The usage of opencv implementation is illustrated in the pictures on the slide.

#Lecture 9
The fundamental matrix expresses that corresponding points lie on
their epipolar lines. 0 = q2'*F*q1; We can estimate fundamental matrix by linear algorithm. It has 8 degrees of freedom and single pair fixes one of them thus we need at least 8 pairs for estimation. There are variations of this algorithm which can estimate the matrix from less points, like five but we don't go into that, bc performance while learning is not a huge concern. In order to use RANSAC we need some sort of distance measure to estimate how good is the current sample. Sampsonâ€™s Distance is what we end up using.

#Lecture 10
Homography is point to point mapping between two planes. How can we use it for images? - we assume that camera doesn't move, only rotates thus the picture is essentially a plane in the world. Rotation doesn't introduce any perspective deformations. It is equivalent to looking at a painting of the world. So we assume the world is flat. The right Homography can be estimated by RANSAC, but we need an error measure. For this pupose we map both points in a pair back and forth and just add the differences between expected ones. With it we can warp the image to the current field of view so two images looks as they are viewed from the same view point. Generate all new x,y coordinates for all pixels in the reference image and use interpolation to computer the value the transformed pixel locations. Combine overlapping values by simple average or median.

#Lecture 11
In order to do VO, we first match some keypoint from one image to the other. As we have those correspondances we can estimate Essential matrix. E = R[t]_{x} it contains all the extrinsic parameters and traslation is up to scale. Thus we have 5 degrees of freedom, we need at least 5 point pairs to estimate it with RANSAC. Not possible to accomplish with linear algorithm, could be done with five-point algo or simply RANSAC. Ok, so we have pose of the second camera relative to the first one, how do we go for more frames and add those translation vectors together? Doing the same procedure will give us new arbitrary scaled translation? Cannot get this point :(. At least the presented idea was to triangulate points from the first pair and later use PnP to estimate the pose of the second camera frame by 2D=>3D correspondances. PnP is concerned with estimating pose of the camera, [R|t] has 6 degrees of freedom. One pair fixes 2, so in theory we should be able to do it with 3 pairs and why the most prominent case of PnP is P3P. In practice fouth point is used to select from four possible solutions. Use of RANSAC is also possible to make it robuts. Three 2D point give three angles and 3D point give three distances. Using simple geometry relations we can calculate the pose.

#Lecture 13
The last topic was about structured light and 3D reconstruction with a help of it. In short structured light is a specific pattern shined on an object (with a laser projecting a plane) which can be used, for example, to match points between two images in stereo pair and later triangulate the 3D points on an object achieving 3D reconstruction. This setup would require stereo setup, we can do it with a pair of laser and camera that are calibrated againts each other, laser can shine a plane and by pixel plane correspondance points in 3D can be traingulated. It's slow because we need a picture per line. Way better option is to go for surface encoding, like shining a pattern (for instance a sin wave) by grating a laser beam and using color intensity or sin pattern of phase shift. In the exercise we have used the latter. One problem arising with phase encoding is that it's repeating thus not unique at all points of the image. To solve this we can take two different phase shifted encoding and subtract them so we get nice unique linear pattern along whole image. FFT is used to parse the phases, uwrapping for making the pattern unique. 

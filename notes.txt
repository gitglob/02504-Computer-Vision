#Lecture 1
Homogeneous coordinates extend the standard system with one additional scale dimension. This has desirable properties like being able to combine rotation & translation into a single transformation matrix which operates on homogeneous points by single multiplication. Also this allows to represent lines in a convenient way so we can get line intersection by cross product of two lines and distance to a point by doing a dot product with a line. Converting is just dividing by the scale parameter, the last element in the vector. Pinhole camera model can be explained simply from the projection matrix P = K[R|t] = KT containing camera intrinsic & extrinsic parameters in K & T respectively. Extrinsics represent camera pose in the  world while intrinsics explain how the projection is done to a camera plane.

#Lecture 2

#Lecture 3

#Lecture 4
DLT (Direct linear transform) can be used to calibrate a camera. It's the same principle as was used in the previous weeks to estimate homography or fundamental matrices. By calibration here I mean finding extrinsic parameters (ones that transform world coordinates to camera reference frame) and intrinsic (whose transform points from the camera coordinate system to pixel coordinate system). We can estimate projection matrix P by using DLT algorithm. This requires to have n pairs of points (3D => 2D pixels correspondences), single pair introduces 2D constrains on X,Y (the third dim is just scale). How many n? P has degrees of freedom: 5 of camera matrix f,alpha,beta,deltax,deltay, 3 rotation params, 3 translation params, 1 for scale. In total 12 degrees of freedom, having in mind that one pair has 2 constraints we need at least 6 points in order to do that. Zhang's method for camera calibration. If we have a known list of 3D points all lying in a plane we can determine homographies giving the mapping between image plane and the one in 3D space. Because we can create connections between 2D camera pixels and known pattern in free space by knowing predefined pattern (with known distances). Checkerboard is commonly used as a 3D plane but basically any other plane could be used with predefined pattern. First we assume all corners are in the Z = 0 plane. So the points in 3D look like Q = [X,Y,0]. Later we can simplify projection equation by throwing out the first column from transformation matrix (Z = 0). This leaves us with homography mapping q = HQ. As explained before we know the correspondences so we can collect a bunch of sample pairs and solve it by DLT as we did with previously. By some clever math we can extract constraints on camera matrix. When we have K it's trivial to compute transformation parameters. Camera parameters again have 12 degrees of freedom so in theory it should be possible top estimate all of the from 6 pairs. In practice it doesn't give good enough results and it's advised to use way more. Some people do it with 2000+ pictures : ). Calibration can also be frames as non-linear least squares problem optimizing over all the camera & distortion parameters by reducing the distance between observations and projections.

#Lecture 5
In computer vision we encounter many problems that can be phrased as non-linear least squares: homography estimation, camera calibration with distortion model, triangulation. There are many algorithm to choose for the problem but Levenberg-Marquardt is often well suited to the types of problems we are likely to encounter in computer vision. The problem is posed as: error = ||f(x)||^2_2. The step of the algorithm goes as: (J'J + lambda*I)*delta = -J*f(x_k). As lambda is increased we just take a small step towards negative gradient direction. But if it's close to the minimum second order approximation of error is relatively good and we can decrease lambda as we get closer. The easiest way to get a Jacobian of a function is to simply use forward, backward difference although it's not very accurate for the learning purpose it was more than enough. In case analytical form can be derived it would more preferable or one can you automatic differentiation tools, like pytorch or many other so you can get derivatives essentially for free, program tracks the operations under the hood and gradients are provided by extrapolating those simple underlying instructions like multiplication, addition, etc.

#Lecture 6
Image correspondence is a problem of finding unique pixel matches between two images capturing the same object from a different perspectives. In order for them to be unique they need a reasonable descriptions. Some simple features to identify them: harris corners, canny edges but in practice you would use more robust features SIFT, SURF, etc. To achieve more reasonable result we focus on the area around each point and try to create meaningful area description which is more resilient to noise bc it's computer over larger area - more information. Gaussian filter is one of the filtering techniques to smoothen out an image. Also it's useful bc we know analytical derivatives of the function. For example Harris corner detector is based on this. We can find averaged Hessian dependant on Gaussian kernel and corners have large values of the metric (det and trace of C(x,y)) regardless of the direction of travel.

#Lecture 7
We have some techniques to fit models. The simpler one is Hough transform – r,theta representation. Idea: we can sample our original image and draw all possible lines going through that point in Hough Space. Do that for all points, the maximum intensity will represent line in original image. We can generalize to more complex models, but it becomes impractical for more than three degrees. RANSAC is based on assumption that by trying to randomly sample some data points and trying to fit the model on each sample we eventually will arrive at some reasonable model which represents the data overall. The approach is outlier resistant and very robust.

#Lecture 8
BLOB - Binary large objects are basically dark areas surrounded by brighter intensities or other way around bright areas surrounded by dark ones. For identifying these second order derivative or just Hessian can be used to determine the curvature of intensities at a point. Similar to Harris corner detector the measure could be represented by trace or determinant of Hessian. In this case we use trace(H) and this can be approximated by difference of Gaussians (DoG). This is a method of taking different levels of blur with Gaussian kernel over the same image and later subtracting those scales, taking out extrema, use non-maximum suppression to make it more robust and woahhla! SIFT features are more complicated, we have not implemented those, but BLOB detection is the first step in that algorithm. The usage of opencv implementation is illustrated in the pictures on the slide.

#Lecture 9
The fundamental matrix expresses that corresponding points lie on
their epipolar lines. 0 = q2'*F*q1; We can estimate fundamental matrix by linear algorithm. It has 8 degrees of freedom and single pair fixes one of them thus we need at least 8 pairs for estimation. There are variations of this algorithm which can estimate the matrix from less points, like five but we don't go into that, bc performance while learning is not a huge concern. In order to use RANSAC we need some sort of distance measure to estimate how good is the current sample. Sampson’s Distance is what we end up using.

#Lecture 10
Homography is point to point mapping between two planes. How can we use it for images? - we assume that camera doesn't move, only rotates thus the picture is essentially a plane in the world. Rotation doesn't introduce any perspective deformations. It is equivalent to looking at a painting of the world. So we assume the world is flat. The right Homography can be estimated by RANSAC, but we need an error measure. For this pupose we map both points in a pair back and forth and just add the differences between expected ones. With it we can warp the image to the current field of view so two images looks as they are viewed from the same view point. Generate all new x,y coordinates for all pixels in the reference image and use interpolation to computer the value the transformed pixel locations. Combine overlapping values by simple average or median.

#Lecture 11

#Lecture 12